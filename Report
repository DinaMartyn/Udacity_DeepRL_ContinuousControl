# Report to the Continuous Control project

This report is written as a part of the Continuous Control project of the [Deep Reinforcement Learning Udacity Nanodegree Program](https://eu.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).

The task is episodic, and in order to solve the environment, the agent must get an average score of +30 over 100 consecutive episodes.

## Learning Algorithm

The applied learning algorithm DDPG was adopted from [ddpg-pendulum](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum) project.
Both Actor and Critic networks have 2 fully connected layers with 384 in the first layer and 192 nodes in the second layer. 
The following hyperparameters are chosen for training:
```
BUFFER_SIZE = int(1e6)  # replay buffer size
BATCH_SIZE = 128        # minibatch size
GAMMA = 0.99            # discount factor
TAU = 1e-3              # for soft update of target parameters
LR_ACTOR = 2e-4         # Learning rate of the actor
LR_CRITIC = 2e-4        # Learning rate of the critic
WEIGHT_DECAY = 0        # L2 weight decay
UPDATE_EVERY = 2        # Update the network after this many steps
NUM_BATCHES = 1         # Roll out this many batches when training
```
`checkpoint_actor_solved.pth` is a file with the saved model weights of the successful actor network of the agent.
`checkpoint_critic_solved.pth` is a file with the saved model weights of the successful critic network of the agent.

## Plot of Rewards during learning

The following scores show that 1866 episodes are enough to train the network in 47697 sec (13,2 hours).
![plot](images/Score.JPG "Score")
![plot](images/Cont_score_good.JPG "Learning performance plot")

A plot illustrates that the agent is able to receive an average reward (over 100 episodes) of at least +30. 

## Ideas for Future Work

The following improvements can follow:
1. The learning takes a significant amount of time. The arcitecture with 128 nodes in the first layer and 63 nodes in the second layer could learn faster, but tried version with this architecture did not provide any good result as can be seen from following picture.
![plot](images/Cont_score_bad.JPG "Bad learning performance plot")
Other combinations of parameters that allows this network reach a desired performance could be a solution to the time problem.
2. Some improvements like algorithms like PPO, A3C, and D4PG and similar can improve the agent's performance if 20 agents are considered.
